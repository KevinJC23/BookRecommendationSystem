# -*- coding: utf-8 -*-
"""BookRecommendationSystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MYm8A16ipMgOw1RZh3w4XkRlkI02bUlW

## **IMPORT LIBRARY/PACKAGES**

Import requisite libraries/packages that will be used.
"""

import os
import csv
import json
import shutil
import kagglehub
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

"""## **DATA PREPARATION**

Download the latest version of Amazon Books Reviews dataset from Kaggle via Kagglehub. Once downloaded, dataset will be located within the specific file path.
"""

path = kagglehub.dataset_download("mohamedbakhet/amazon-books-reviews")
print("Path to dataset files:", path)

"""Replace and adjust previously acquired dataset within a specific location."""

target_path = '/content/amazon-books-reviews-dataset'

if not os.path.exists(target_path):
    shutil.move(path, target_path)

"""This dataset folder consist of two CSV files, namely ```Books_rating.csv``` and ```books_data.csv```. For this purpose, we will utilize ```Books_rating.csv``` and assign pandas CSV reading function to the ```books``` variable to facilitate examination of CSV's metadata, such as its columns headers and row count.  """

books = pd.read_csv('/content/amazon-books-reviews-dataset/Books_rating.csv')
books

"""Due to an overwhelming amount of rows —approximately 3,000,000— and limited resource to manage the data. We extracted 25,000 random sample entries to simplify the dataset numbers."""

books = books.sample(n=25000, random_state=42)
books = books.reset_index(drop=True)
books

"""Select the desired columns and assign them into ```cf_books``` variable."""

cf_books = books[['Id', 'Title', 'User_id', 'review/score']].dropna()
cf_books

"""## **EXPLORATORY DATA ANALYSIS (EDA)**

Call ```.info()``` method to obtain a summary of the dataset's column structure.
"""

cf_books.info()

"""Highlight four columns, where the ```review/score``` is categorized as float datatypes and the remaining columns are categorized as object datatypes.

Call ```.describe()``` method to obtain a summary of descriptive statistics for the float datatypes column, specifically ```review/score```.
"""

cf_books.describe()

"""Considering this summary, since there are no issues with the ```review/score``` columns. So, we can proceed to further exploration.

Check missing value in each column using ```.isnull().sum()``` method.
"""

cf_books.isnull().sum()

"""Based on this result, the analysis of these outcomes reveals no missing data within any of the columns, so removing the missing values is an irrelevant procedure.

Check data duplication using ```.duplicated().sum()``` method.
"""

cf_books.duplicated().sum()

"""The result shows only 4 rows with duplicated data, so the removal of these duplicate entries may be necessary.

Display unique book titles in the dataset using ```.unique()``` methods and count the number of titles using ```len()```.
"""

unique_title = cf_books['Title'].unique()
print(f"Unique Values: {len(unique_title)}")

"""To summarize, the total of the titles is 12,551 and this level of variety is beneficial for generating book recommendations.

Display value counts of  each ```review/score``` and visualize distribution using boxplot.
"""

print(cf_books['review/score'].value_counts().sort_index())

sns.boxplot(x=cf_books['review/score'])
plt.title('Book Rating Distribution')
plt.xlabel('Rating')
plt.show()

"""Based on value counts, a rating of 5 appears most frequently, while rating of 2 is the least common. In the boxplot, distribution of ```review/score``` is positively skewed (right-skewed). However, since the data will be used to generate recommendations that prioritize higher ratings, removing outlier may not be necessary.

## **DATA PREPROCESSING**

Eleminate entries with duplicated values that were identified earlier.
"""

cf_books = cf_books.drop_duplicates()

"""Order the dataset alpahabetically by the ```Title``` columns."""

cf_books_sort = cf_books.sort_values(by='Title')
cf_books_sort

"""Display the distinct values from the ```Title``` columns."""

cf_books_sort.Title.unique()

"""## **MODEL DEVELOPMENT**

### **CONTENT BASED FILTERING**

Apply a TF-IDF vectorizer to extract important feature representations from each book title, then fitting and transforming the text into a numerical matrices.
"""

tfidf = TfidfVectorizer(stop_words='english')
tfidf_matrices = tfidf.fit_transform(cf_books_sort['Title'])

tfidf.get_feature_names_out()

tfidf_matrices.shape

"""The resulting matrices has a shape of (20381, 14039), with each row representing a book title and each column corresponding to a unique term extratced from the titles.

Since the output of TF-IDF vectorizer is a sparse matrices, we convert it into a dense format using ```.dense()```.
"""

tfidf_matrices.todense()

"""Display TF-IDF matrices for the first 10 rows of the ```Title``` column. Words that appear in these titles are assigned as a non-zero weights by TF-IDF vectorizer, reflecting their relative importance within the corpus.  """

pd.DataFrame(
    tfidf_matrices.todense(),
    columns=tfidf.get_feature_names_out(),
    index=cf_books_sort['Title'],
).sample(22, axis=1).sample(10, axis=0)

"""Calculate similarity degree between each title using Cosine Similarity technique."""

cosine_sim = cosine_similarity(tfidf_matrices)
cosine_sim

"""Display a subset of the similarity matrices for each title using 5 sample columns (axis=1) and 100 sample rows (axis=0)."""

cosine_sim_books = pd.DataFrame(
    cosine_sim,
    index=cf_books_sort['Title'],
    columns=cf_books_sort['Title'],
)

print('Shape:', cosine_sim_books.shape)
cosine_sim_books.sample(5, axis=1).sample(100, axis=0)

"""Based on this result, the cosine similarity matrices has a shape (20381, 20381), where both x-axis and y-axis represent book titles. This allow us to identify similarity between titles. For example, ```Elmer Gantry (Avon Books #1)``` shows similary to ```How to Think About Statistics: Sixth Edition (Series of Books in Psychology)``` with similary score up to 0.097667.

### **COLLABORATIVE FILTERING**

Convert ```User_id``` and ```Id``` into encoded integer representations.
"""

user_ids = cf_books_sort['User_id'].unique().tolist()
print('List UserID:', user_ids)

user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
print('Encoded UserID:', user_to_user_encoded)

user_encode_to_user = {i: x for i, x in enumerate(user_ids)}
print('Encode Number of UserID:', user_encode_to_user)

ids = cf_books_sort['Id'].unique().tolist()
print('List Id:', ids)

id_to_id_encode = {x: i for i, x in enumerate(ids)}
print('Encoded Id:', id_to_id_encode)

id_encode_to_id = {i: x for i, x in enumerate(ids)}
print('Encode Number to Id:', id_encode_to_id)

"""Associate ```User_id``` and ```Id``` columns with the corresponding DataFrame."""

cf_books_sort['user_id'] = cf_books_sort['User_id'].map(user_to_user_encoded)
cf_books_sort['title_id'] = cf_books_sort['Id'].map(id_to_id_encode)

"""Retrieve the count of unique UserID and TitleID, as well as minimum and maximum rating values."""

num_user_id = len(user_to_user_encoded)
print(f'UserID: {num_user_id}')

num_title_id = len(id_to_id_encode)
print(f'TitleID: {num_title_id}')

min_rating = min(cf_books_sort['review/score'])
print(f'Min: {min_rating}')

max_rating = max(cf_books_sort['review/score'])
print(f'Max: {max_rating}')

"""Before splitting, shuffle dataset to ensure random distribution accros the training and validation sets."""

cf_books_sort = cf_books_sort.sample(frac=1, random_state=42)
cf_books_sort

"""Define ```x``` for features and ```y``` as a target variable, then split the data into 80% training and 20% for validation. After that, normalize ratings to a 0-1 range to facilitate model convergence."""

x = cf_books_sort[['user_id', 'title_id']].values
y = cf_books_sort['review/score'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

train_indices = int(0.8 * cf_books_sort.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)

print(x, y)

"""Define a ```RecommenderNet``` class that embeds both of ```UserID``` and ```TitleID``` columns, computes their with dot product, and incorporates individual bias terms. The similarity score is passed through a sigmoid activation function to normalize values between 0 and 1.   """

class RecommenderNet(tf.keras.Model):
    def __init__(self, num_user_id, num_title_id, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)
        self.num_user_id = num_user_id
        self.num_title_id = num_title_id
        self.embedding_size = embedding_size

        self.user_id_embedding = layers.Embedding(
            num_user_id,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.user_id_bias = layers.Embedding(
            num_user_id,
            1,
        )

        self.title_id_embedding = layers.Embedding(
            num_title_id,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        )
        self.title_id_bias = layers.Embedding(num_title_id, 1)

    def call(self, inputs):
        user_id_vector = self.user_id_embedding(inputs[:, 0])
        user_id_bias = self.user_id_bias(inputs[:, 0])

        title_id_vector = self.title_id_embedding(inputs[:, 1])
        title_id_bias = self.title_id_bias(inputs[:, 1])

        dot_user_id_and_title_id = tf.tensordot(user_id_vector, title_id_vector, 2)

        x = dot_user_id_and_title_id + user_id_bias + title_id_bias

        return tf.nn.sigmoid(x)

"""Initialize compilation of the model by defining ```binary cross-entropy``` loss, ```Adam``` optimizer, and ```RMSE``` as the evaluation metric."""

model = RecommenderNet(num_user_id, num_title_id, 50)

model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""Define a callback and initiate the model training process."""

callback = tf.keras.callbacks.EarlyStopping(
    monitor='val_root_mean_squared_error',
    patience=10,
    restore_best_weights=True
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=8,
    epochs=100,
    validation_data=(x_val, y_val),
    callbacks=[callback]
)

"""## **EVALUATION**

### **CONTENT BASED FILTERING**

A query book is selected as the input for recommendation function. A set of manually curated relevant titles is defined to benchmark recommendation quality, then retrieve the 5 most similar titles based on the query.
"""

query_book = 'Buddha Mom: The Journey Through Mindful Mothering'

relevant_books = [
    'My Mom Is a Dragon',
    'The Buddha in the robot',
    'The Buddha of Suburbia',
]

recommended_df = books_recommendations(query_book, k=5)
recommended_books = recommended_df['Title'].tolist()

"""Evaluation metrics such as ```Precision@k```, ```Recall@k```, ```F1@k```, and ```MRR``` are essential for measuring effectiveness of recommender system in terms of both relevance and ranking performance."""

def precision_at_k(recommended, relevant, k=5):
    recommended_k = recommended[:k]
    hits = sum([1 for book in recommended_k if book in relevant])
    return hits / k

precision = precision_at_k(recommended_books, relevant_books, k=5)
print('Precision@5:', precision)

def recall_at_k(recommended, relevant, k=5):
    recommended_k = recommended[:k]
    hits = sum(1 for book in recommended_k if book in relevant)
    return hits/len(relevant) if len(relevant) > 0 else 0

recall = recall_at_k(recommended_books, relevant_books, k=5)
print(f'Recall@5:', recall)

def f1_at_k(recommended, relevant, k=5):
    if precision + recall == 0:
        return 0
    return 2 * (precision * recall) / (precision + recall)

f1 = f1_at_k(precision, recall)
print(f'F1@5:', f1)

def reciprocal_rank(recommended, relevant):
    for i, book in enumerate(recommended):
        if book in relevant:
            return 1 / (i + 1)
    return 0

mrr = reciprocal_rank(recommended_books, relevant_books)
print('MRR:', mrr)

"""These result suggest that recommendation system is highly effective, especially in recalling all relevant items and ranking them prominetly. Despite a slightly imperfect precision, this system still provides strong overall performance.

### **COLLABORATIVE FILTERING**

Following the training phase, model's learning performance is visualized through a line plot of Root Mean Square Error (RMSE) across epochs, comparing both training dan validation data.
"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""This graph indicates that model performs increasingly well on training data, while its performance on validation data remains relatively constant. This suggest a risk of overfitting, as model fails to generalize to unseen data.

## **RESULT**

### **CONTENT BASED FILTERING**

Define a function to returns top *k* book titles most similar to a given input title based on Cosine Similarity and enables content-based book recommendations by analyzing textual similarity among books titles. It excludes original input title from results and merges recommended titles with their associated metadata.
"""

def books_recommendations(Title, similarity_data=cosine_sim_books, items=cf_books_sort[['Title', 'Id']], k=5):
    index = similarity_data.loc[:, Title].to_numpy().argpartition(
        range(-1, -k, -1))

    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    closest = closest.drop(Title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

"""This function returns the top 5 books that are most similar to "*Buddha Mom: The Journey Through Mindful Mothering*" in terms of their TF-IDF feature representation using Cosine Similarity."""

books_recommendations('Buddha Mom: The Journey Through Mindful Mothering')

"""### **COLLABORATIVE FILTERING**

Selects a random user from dataset and retrieves all the books they have rated. By filtering dataset based on the selected UserID, we can examine the titles and corresponding scores provided by user.
"""

df_rating = cf_books_sort.copy()
random_user_id = df_rating['User_id'].sample(1).iloc[0]
books_rated_by_user = df_rating[df_rating['User_id'] == random_user_id]

print(f"User {random_user_id} Has Rated the Following Books:")
print(books_rated_by_user[['Title', 'review/score']])

"""This system identifies all books that user hasn't rated and predicts the user potential interest in each on using a trained recommender model. Based on predicted scores, system ranks and selects top 10 books likely to match user's preferences. These recommendation are then displayed along with their title and existing average scores."""

all_book_ids = set(id_to_id_encode.keys())
rated_book_ids = set(books_rated_by_user['Id'].values)
unrated_book_ids = list(all_book_ids - rated_book_ids)
unrated_book_encoded = [id_to_id_encode[b] for b in unrated_book_ids]
user_encoded = user_to_user_encoded[random_user_id]

user_input = np.array([[user_encoded]] * len(unrated_book_encoded))
book_input = np.array(unrated_book_encoded).reshape(-1, 1)
model_input = np.hstack([user_input, book_input])

predicted_scores = model.predict(model_input).flatten()

top_indices = predicted_scores.argsort()[-10:][::-1]
top_book_encoded = [unrated_book_encoded[i] for i in top_indices]
top_book_ids = [id_encode_to_id[enc] for enc in top_book_encoded]

recommended_books = df_rating[df_rating['Id'].isin(top_book_ids)][['Title', 'review/score']].drop_duplicates()
print("\nTop Book Recommendation For {}:".format(random_user_id))
print(recommended_books.reset_index(drop=True))

"""Based on the result, the system recommend user's by book title such as "A Christmas Carol" and "The Richest Man in Babylon", each with high predicted interest. Despite variations in actual dataset ratings, these books were ranked highly by model due to their relevance to user's profile and reading history."""